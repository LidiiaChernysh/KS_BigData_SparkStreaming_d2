{"cells":[{"cell_type":"code","source":["#from pyspark.sql.functions import unbase64\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n# Source with default settings\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\n\n# Prepare training data from a list of (label, features) tuples.\ntraining = spark.createDataFrame([\n    (1.0, Vectors.dense([50.0, 20.0, 20.0])),\n    (0.0, Vectors.dense([60.0, 30.0, 30.0])),\n    (0.0, Vectors.dense([70, 25.0, 250.0])),\n    (1.0, Vectors.dense([65, 18.0, 18.0]))], [\"label\", \"features\"])\n\n# Create a LogisticRegression instance. This instance is an Estimator.\nlr = LogisticRegression(maxIter=10, regParam=0.01)\n# Print out the parameters, documentation, and any default values.\nprint(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n\n# Learn a LogisticRegression model. This uses the parameters stored in lr.\nmodel1 = lr.fit(training)\n\n# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint(\"Model 1 was fit using parameters: \")\nprint(model1.extractParamMap())\n\n# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap = {lr.maxIter: 20}\nparamMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\nparamMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n\n# You can combine paramMaps, which are python dictionaries.\nparamMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\nparamMapCombined = paramMap.copy()\nparamMapCombined.update(paramMap2)\n\n# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2 = lr.fit(training, paramMapCombined)\nprint(\"Model 2 was fit using parameters: \")\nprint(model2.extractParamMap())\n\n\n\n#connectionString = \"Endpoint=sb://iothub-ns-ks-bigdata-5809426-7048d14abf.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=sQQPHOQhXdvNxTBcBjTfz2fWOTTTnRwRZu+/9HKbXmM=;EntityPath=ks-bigdata-iot-hub-test\"\n# connectionString = \"Endpoint=sb://iothub-ns-ks-bigdata-5809426-7048d14abf.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=sQQPHOQhXdvNxTBcBjTfz2fWOTTTnRwRZu+/9HKbXmM=;EntityPath=ks-bigdata-iot-hub-test\"\n\nconnectionString = \"Endpoint=sb://iothub-ns-iot01chern-5953780-353bfe5217.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=FSdANoPPmFQw46Xs33GkbEi1IJE9Posq4LLmAv2Dsis=;EntityPath=iot01chernysh\"\n\nehConf = {\n  'eventhubs.connectionString' : connectionString\n}\n\nehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n\n# Schema of incoming data from IoT hub\nschema = \"timestamp timestamp, temperature double, humidity double, deviceId string, status string\"\n\n# Read directly from IoT Hub using the EventHubs library for Databricks\niot_stream = (\n  spark.readStream.format(\"eventhubs\")                                              # Read from IoT Hubs directly\n    .options(**ehConf)                                                              # Use the Event-Hub-enabled connect \n    .load()                                                                         # Load the data\n    .withColumn('reading', F.from_json(F.col('body').cast('string'), schema))       # Extract the \"body\" payload from  \n)\n\n#display(iot_stream)\n\niot_stream = iot_stream.withColumn('temperature', iot_stream.reading.temperature).withColumn('humidity', iot_stream.reading.humidity).withColumn('deviceId', iot_stream.reading.deviceId).withColumn('ts', iot_stream.reading.timestamp)\niot_stream2 = iot_stream.groupBy(window(iot_stream.ts, \"5 seconds\", \"3 seconds\"), iot_stream.deviceId).agg(avg('humidity'), avg('temperature'), min('humidity'), min('temperature'), max('humidity'), max('temperature'))\n\nassembler = VectorAssembler(\n    inputCols=[\"avg(humidity)\", 'avg(temperature)', 'avg(temperature)'],\n    outputCol=\"features\")\n\nfeatures = assembler.transform(iot_stream2.withColumn('tsp', iot_stream2.window.start))\n#print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n#output.select(\"features\", \"clicked\").show(truncate=False)\n\npredictions = model1.transform(features)\npredictions.printSchema\ndisplay(predictions)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"965b7416-6a75-469e-81bf-a20330163e14"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"plotlyLine","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":false},{"key":"stacked","value":true},{"key":"100_stacked","value":false}],"plotlyLine":[{"key":"yRange","value":""},{"key":"showPoints","value":false},{"key":"logScale","value":false}]},"pivotColumns":[],"pivotAggregation":"sum","xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"window","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"deviceId","type":"\"string\"","metadata":"{}"},{"name":"avg(humidity)","type":"\"double\"","metadata":"{}"},{"name":"avg(temperature)","type":"\"double\"","metadata":"{}"},{"name":"min(humidity)","type":"\"double\"","metadata":"{}"},{"name":"min(temperature)","type":"\"double\"","metadata":"{}"},{"name":"max(humidity)","type":"\"double\"","metadata":"{}"},{"name":"max(temperature)","type":"\"double\"","metadata":"{}"},{"name":"tsp","type":"\"timestamp\"","metadata":"{}"},{"name":"features","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"avg(humidity)\"},{\"idx\":1,\"name\":\"avg(temperature)\"},{\"idx\":2,\"name\":\"avg(temperature)\"}]},\"num_attrs\":3}}"},{"name":"rawPrediction","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"num_attrs\":2}}"},{"name":"probability","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"num_attrs\":2}}"},{"name":"prediction","type":"\"double\"","metadata":"{\"ml_attr\":{\"type\":\"nominal\",\"num_vals\":2}}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>window</th><th>deviceId</th><th>avg(humidity)</th><th>avg(temperature)</th><th>min(humidity)</th><th>min(temperature)</th><th>max(humidity)</th><th>max(temperature)</th><th>tsp</th><th>features</th><th>rawPrediction</th><th>probability</th><th>prediction</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"KS_BigData_SparkStreaming_d2","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1596470497699125}},"nbformat":4,"nbformat_minor":0}
